\documentclass[conference]{IEEEtran}

\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{enumitem}
\usepackage[hidelinks]{hyperref} % It's often best to load hyperref last

\begin{document}

\title{Loan Anomaly Detection for Repayment Behaviour Analysis\\
using Unsupervised Ensemble Methods}

\author{
\IEEEauthorblockN{Himanshu Maithani \quad Roheth Balamurugan}
\IEEEauthorblockA{CS5344 Big Data Analytics -- Track 2: Finance, Group 11\\
National University of Singapore\\
Email: \{eXXXXXXX, eXXXXXXX\}@u.nus.edu}
}

\maketitle

\begin{abstract}
Accurately detecting abnormal loan repayment behaviour is critical for credit risk management.
In this project we study a \emph{semi-supervised} anomaly detection problem based on the Freddie Mac Single-Family Loan-Level dataset, where the training set contains only normal loans, while validation and test sets contain a mixture of normal and abnormal loans.
Our goal is to assign an anomaly score to each loan and maximise the Average Precision (AUPRC) on the highly imbalanced validation set.

We design a domain-aware feature engineering pipeline that combines static origination variables with short performance panels over 14 months.
On top of this feature space we build and evaluate a diverse pool of unsupervised detectors, including amortisation-based detectors, Random-Forest Outlier Detection (RFOD), correlation-anomaly detection, random-projection LOF, KMeans+cohort LOF, multi-$k$ distance measures, one-class SVM, Isolation Forest and PCA reconstruction error.
Our final hybrid ensemble fuses the most predictive and complementary detectors (Amortisation, RFOD, Cohort LOF, RP-LOF, and OCSVM) using rank-based and amortisation-gated strategies tuned on validation labels.

Compared to classical baselines such as LOF, PCA and Isolation Forest on raw features, our final ensemble more than doubles the AUPRC while respecting the competition constraint that only the training set may be used to fit models.
We further apply \emph{unsupervised explainability} using permutation importance on an Isolation Forest surrogate, identifying the key engineered features that drive anomaly scores.
\end{abstract}

% ============================================================
\section{Introduction}
The modern mortgage market generates massive streams of high-dimensional data at the individual-loan level.
Lenders must continuously monitor portfolios and detect loans whose repayment behaviour deviates from normal patterns, as these may signal impending default, restructuring, or operational issues.
Traditional credit scoring focuses mainly on static borrower attributes at origination, which may miss subtle dynamics visible in early repayment history.

In this project we treat loan-level default risk as an \emph{anomaly detection} problem.
We work in a semi-supervised setting: the training data are known to contain only normal loans, whereas the validation and test sets contain both normal and abnormal loans.
The labels are revealed only for validation, and the official evaluation metric is Average Precision (AUPRC) on the anomaly class.

Our approach evolves from a simple Local Outlier Factor (LOF) baseline to a rich ensemble of unsupervised detectors operating on a domain-informed feature space.
A key finding is that \emph{good feature engineering and smart fusion are more important than a single sophisticated model}, especially in the presence of strong constraints on label usage.

% ============================================================
\section{Target Task, Motivation and Dataset}
\subsection{Target Task}
The target task is \textbf{semi-supervised anomaly detection} on loan repayment behaviour.
Formally, each loan $i$ is represented as
\[
x_i = (s_i, r_{i,0}, r_{i,1}, \dots, r_{i,13}),
\]
where $s_i$ denotes static origination variables (borrower credit quality, loan terms, property information) and $r_{i,t}$ captures the monthly performance vector at month $t$ (unpaid balance, interest rate, estimated LTV, etc.).
Each loan has a binary label $y_i \in \{0,1\}$, with $0$ = normal and $1$ = abnormal.

The competition provides:
\begin{itemize}[leftmargin=*]
    \item \textbf{Train:} 30{,}504 loans, all normal ($y=0$).
    \item \textbf{Validation:} 5{,}370 loans, of which 677 (12.61\%) are abnormal.
    \item \textbf{Test:} 13{,}426 loans, unlabeled.
\end{itemize}
We must output a continuous anomaly score for each validation/test loan.
Models may be \emph{trained} only on the normal training set, but validation labels may be used for hyperparameter and fusion selection.
The main metric is AUPRC on the positive (abnormal) class.

\subsection{Motivation}
From a business perspective, early detection of anomalous repayment patterns enables:
\begin{itemize}[leftmargin=*]
    \item \textbf{Proactive risk management:} identify loans likely to default or require restructuring.
    \item \textbf{Resource allocation:} focus manual review on a small set of high-risk loans.
    \item \textbf{Model robustness:} anomaly scores can complement traditional PD (probability of default) models and stress-testing.
\end{itemize}
From a machine learning perspective, the setting is interesting because:
\begin{itemize}[leftmargin=*]
    \item Only normal labels are available for training, pushing us toward unsupervised or one-class methods.
    \item The data combines static, categorical, and temporal components, requiring careful feature engineering.
    \item The dataset is highly imbalanced, so standard accuracy is uninformative; AUPRC is more sensitive to performance on rare anomalies.
\end{itemize}

\subsection{Dataset and Feature Structure}
The dataset is derived from the Freddie Mac Single-Family Loan-Level dataset~\cite{freddie}.
Each row corresponds to a single loan.
The columns can be grouped as follows:

\textbf{Static origination variables (31 columns).}
These describe borrower and loan characteristics at origination:
\begin{itemize}[leftmargin=*]
    \item CreditScore, OriginalLTV, OriginalCLTV, OriginalDTI, OriginalUPB, OriginalInterestRate.
    \item LoanPurpose, ProductType, Channel, OccupancyStatus, NumberOfBorrowers, NumberOfUnits.
    \item PropertyState, PropertyType, PostalCode, MSA.
    \item SellerName, ServicerName, SuperConformingFlag, ProgramIndicator, PropertyValMethod, InterestOnlyFlag, BalloonIndicator.
    \item FirstPaymentDate and MaturityDate (YYYYMM).
\end{itemize}

\textbf{Temporal performance variables (112 columns).}
For each loan we have a short panel of 14 months ($t=0,\dots,13$).
For each month, eight fields are repeated:
\begin{itemize}[leftmargin=*]
    \item CurrentActualUPB, CurrentInterestRate, CurrentNonInterestBearingUPB,
    \item EstimatedLTV, InterestBearingUPB,
    \item LoanAge, MonthlyReportingPeriod, RemainingMonthsToLegalMaturity.
\end{itemize}

\textbf{Data quality characteristics:}
Static features use special codes to represent missingness or ``not applicable'', e.g.\ CreditScore=9999, OriginalDTI=999, OriginalLTV=999, MI\_Pct=999, EstimatedLTV=999, and categorical flags such as ``9'' or ``99'' for ``not available''.
Some fields are almost entirely missing (e.g.\ ReliefRefinanceIndicator, PreHARP Flag, SuperConformingFlag), while temporal panels contain essentially no missing entries.

% ============================================================
\section{Challenges}
We encountered several challenges specific to this task.

\subsection{Semi-Supervised and Imbalanced Setting}
The normal-only training set prevents direct application of supervised classification.
All detectors must be trained without seeing positive examples; labels are used only to rank models on validation.
Furthermore, only 12.61\% of validation loans are abnormal, so methods optimising accuracy can trivially predict ``normal'' and still perform poorly in terms of AUPRC.

\subsection{High-Dimensional Mixed Data}
We work with 145 raw columns combining integers, floats and categorical variables, and after feature engineering the dimensionality grows even further.
Distance-based methods such as kNN/LOF suffer from the curse of dimensionality if applied na\"ively, while categorical variables must be encoded carefully to avoid exploding the feature space.

\subsection{Temporal Modelling under Short Panels}
The performance panel is short (14 months), so we cannot rely on long-term time series models.
However, many anomalies are expressed as \emph{changes over time}: unusual principal paydown, irregular interest rate patterns, or abnormal evolution of estimated LTV.
Extracting informative temporal summaries that generalise across loans is therefore crucial.

\subsection{Domain-Specific Encoding and Sentinels}
Several important risk drivers, such as CreditScore and OriginalDTI, use sentinel codes (9999, 999) for missing values.
Treating these as numeric values would distort distributions and mislead scaling and PCA.
We need domain-aware cleaning that both handles these sentinels and preserves information about missingness.

% ============================================================
\section{Related Work}
Unsupervised anomaly detection in high-dimensional numeric data has been studied extensively.
Aggarwal~\cite{aggarwal2017} surveys clustering and subspace-based methods as well as density-based outlier detectors such as Local Outlier Factor (LOF).
Chandola et al.~\cite{chandola2009} provide a general taxonomy of anomaly detection techniques, including one-class classification, reconstruction-based methods (e.g.\ PCA), and proximity-based models.
Ahmed et al.~\cite{ahmed2016} discuss anomaly detection in network time series, highlighting the importance of combining temporal statistics with static features.
Hybrid ensembles that combine multiple detectors, such as CBLOF with Isolation Forest for credit-card fraud~\cite{chugh2024}, motivated our approach toward a heterogeneous ensemble rather than a single model.

These works suggest that (1) capturing local density structure, (2) modelling temporal evolution, and (3) combining complementary detectors are promising directions for our loan dataset.

% ============================================================
\section{Approach}
Our final system is a multi-stage pipeline, as summarised in Figure~\ref{fig:pipeline_flowchart}.
The approach begins by processing raw data through our \textsf{FeatureBuilderAdvanced}, which creates two distinct, parallel feature streams: (1) the full, scaled feature matrix used for domain-specific detectors and (2) a dense, lower-dimensional PCA embedding used for density-based detectors.
A diverse bank of unsupervised detectors, each tailored to a specific feature stream (e.g., Amortisation Score on scaled features, Cohort LOF on the embedding), generates independent anomaly scores.
These scores are then standardised through a calibration process (ECDF and EVT) and finally combined in our amortisation-gated fusion model to produce a single, robust anomaly score.
We maintain a strict separation between training and validation: \emph{all transformations and detectors are fitted only on the training set}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_pipeline_flowchart.pdf}
    \caption{Anomaly detection pipeline architecture. Raw data is transformed by FeatureBuilderAdvanced into two parallel streams (full scaled features and PCA embedding), which feed a diverse detector bank. Individual detector scores are calibrated and fused using an amortisation-gated strategy to produce the final anomaly score.}
    \label{fig:pipeline_flowchart}
\end{figure}

\subsection{Preprocessing and Static Feature Engineering}

\subsubsection{Sentinel Handling and Missingness}
Following the Freddie Mac documentation, we treat special codes as missing values:
\begin{itemize}[leftmargin=*]
    \item CreditScore=9999, OriginalDTI=999, OriginalLTV=999, MI\_Pct=999 and EstimatedLTV=999 are mapped to NaN.
    \item Categorical flags ``9'' or ``99'' in FirstTimeHomebuyerFlag, OccupancyStatus, LoanPurpose, Channel, PropertyType and ProgramIndicator are treated as ``not available''.
\end{itemize}
For each of these variables we create a binary ``\_missing'' indicator.
This prevents artificial extremes from contaminating scaling and also retains the information that a key field was not reported, which itself may correlate with risk.

\subsubsection{Categorical Encoding and Numeric Imputation}
Categorical variables (e.g.\ LoanPurpose, Channel, PropertyType, SellerName, ServicerName, PropertyState) are label-encoded using only categories observed in the training data.
Unseen categories in validation or test are mapped to an \textsf{UNKNOWN} token.
Numeric static features are imputed with their training median, a robust choice for skewed financial data.

\subsubsection{Domain-Informed Static Risk Features}
In addition to raw origination variables we engineer several static risk ratios that combine credit quality, leverage and payment scale:
\begin{itemize}[leftmargin=*]
    \item \textbf{Credit--LTV ratio}:
    $\text{CreditScore} / (\lvert \text{OriginalLTV}\rvert + 1)$, capturing how much credit quality backs a given level of leverage.
    \item \textbf{DTI--LTV ratio}:
    $\text{OriginalDTI} / (\lvert \text{OriginalLTV}\rvert + 1)$, highlighting borrowers who are simultaneously highly indebted and highly leveraged.
    \item \textbf{UPB--LTV ratio}: $\text{OriginalUPB} / (\lvert \text{OriginalLTV}\rvert + 1)$, reflecting loan size relative to collateral value.
    \item \textbf{Payment burden}: $\text{OriginalUPB} \times \text{monthly interest rate}$, approximating the monthly interest payment the borrower faces.
    \item \textbf{Rate-to-term}: $\text{OriginalInterestRate} / (\lvert \text{OriginalLoanTerm}\rvert + 1)$, coupling price of credit with amortisation horizon.
\end{itemize}
These engineered features give detectors explicit signals about credit affordability beyond raw inputs.

\subsubsection{Scaling and PCA Embedding}
After concatenating static, temporal and amortisation features (Sections~\ref{sec:temporal}--\ref{sec:amort}), we apply a \textbf{RobustScaler} fitted on the training set, which centres each feature at its median and scales by its interquartile range.
This reduces sensitivity to a few extreme observations---precisely what we want for anomaly detection.

We then fit a PCA model with $K=80$ components on the scaled training matrix to obtain a dense, approximately spherical embedding used by several detectors (LOF, KMeans, One-Class SVM, Isolation Forest).
The feature builder remembers index slices for static, temporal and amortisation feature blocks so that detectors can operate on appropriate subsets.

While we observed in baseline testing that a simple LOF model performed better on the full, scaled feature space without PCA (Table~\ref{tab:baseline_comparison}), the PCA embedding proved essential for our more advanced detectors, such as Cohort LOF and RP-LOF.
The strong individual performance of these embedding-based detectors (Section~\ref{sec:final_results}) confirmed the value of the PCA-transformed space for capturing certain anomaly structures, justifying its use as a key input stream for our final ensemble.

\subsection{Temporal Feature Engineering}
\label{sec:temporal}
The performance panel provides 14 monthly snapshots ($N=0$ to $13$).
Instead of working on raw sequences, we summarise the trajectory of key variables using a multi-window strategy.

\subsubsection{Multi-Window Strategy}
Temporal columns are grouped by suffix into types such as EstimatedLTV, InterestBearingUPB, CurrentInterestRate, LoanAge and RemainingMonthsToLegalMaturity.
For each type we consider several sets of month indices:
\begin{itemize}[leftmargin=*]
    \item \textbf{Main (Quarterly):} months $\{0,3,6,9,12\}$;
    \item \textbf{Alt1 (Bimonthly):} months $\{0,2,4,6,8,10,12\}$;
    \item \textbf{Alt2 (First-Year Focus):} months $\{0,3,6,9\}$.
\end{itemize}
Before computing statistics we perform forward- then backward-filling along each loan's timeline to handle rare gaps, ensuring smooth trajectories.

\subsubsection{Core Temporal Features}
For each temporal type and window we compute four core statistics:
\begin{itemize}[leftmargin=*]
    \item \textbf{Trend:} scaled difference between last and first value:
    \[
    \text{Trend} =
    \frac{x_{\text{last}} - x_{\text{first}}}{\lvert x_{\text{first}} \rvert + 1},
    \]
    capturing the overall direction of movement (e.g.\ rising EstimatedLTV).
    \item \textbf{Volatility:} standard deviation relative to mean magnitude:
    \[
    \text{Volatility} =
    \frac{\mathrm{std}(x)}{\lvert \mathrm{mean}(x)\rvert + 1},
    \]
    measuring stability; high volatility in balance or LTV often indicates irregular payments or valuation shocks.
    \item \textbf{First-diff mean}: average relative month-to-month change
    of the windowed series.
    \item \textbf{First-diff std}: variability of these month-to-month
    changes, signalling erratic repayment.
\end{itemize}
In addition, for each temporal type we compute a \textbf{global trend} from month 0 to 13, giving a coarse summary over the full panel.

Particularly important temporal features discovered later via explainability include volatility and trend of EstimatedLTV and InterestBearingUPB, and the trend / dispersion of RemainingMonthsToLegalMaturity and LoanAge, all of which reflect how quickly the loan is paying down and how its collateral position is evolving.

\subsection{Amortisation Shortfall Modelling}
\label{sec:amort}
A central idea of our approach is that normal fixed-rate mortgages follow a predictable amortisation path.
If the observed principal reduction is systematically smaller than expected, the loan may be in distress.

\subsubsection{Expected Monthly Payment}
For each loan and each month we compute the expected monthly payment under standard annuity amortisation using the loan's \emph{current} state:
\[
P_{\text{exp}} = P \times
\frac{r(1+r)^n}{(1+r)^n - 1},
\]
where $P$ is the current interest-bearing balance (InterestBearingUPB), $r$ is the monthly interest rate (CurrentInterestRate/1200) and $n$ is the remaining months to legal maturity.
In edge cases (e.g.\ zero rate) we fall back to dividing the balance evenly over the remaining term.

The expected principal reduction is then
\[
\Delta P_{\text{exp}} = \max(P_{\text{exp}} - rP, 0).
\]

\subsubsection{Observed Principal Reduction and Shortfall}
The observed principal reduction between months $t-1$ and $t$ is
\[
\Delta P_{\text{obs},t} = \max(P_{t-1} - P_t, 0).
\]
We define a principal shortfall ratio
\[
\text{Shortfall}_t =
\frac{\Delta P_{\text{exp},t} - \Delta P_{\text{obs},t}}
     {\lvert \Delta P_{\text{exp},t} \rvert + \varepsilon},
\]
clipped to $[0,1]$ so that 0 means ``on track or better'' and values near 1 mean extremely weak amortisation relative to expectation.

We restrict this calculation to standard fixed-rate, non--interest-only, non-balloon loans using ProductType, InterestOnlyFlag and BalloonIndicator.
For others we set the shortfall statistics to zero and provide a mask feature.

\subsubsection{Engineered Amortisation Features}
From the shortfall sequence we derive several features:
\begin{itemize}[leftmargin=*]
    \item \textbf{amort\_short\_mean}: mean shortfall across months, capturing persistent underpayment.
    \item \textbf{amort\_short\_70}: fraction of months with shortfall $>70\%$, indicating severe stress.
    \item \textbf{amort\_short\_50}: fraction of months with shortfall $>50\%$, indicating moderate stress.
    \item \textbf{amort\_mask\_not\_applicable}: flag for loans where amortisation logic does not apply.
\end{itemize}
These amortisation features are both highly predictive and easy to explain to risk managers, and they play a central role in our final ensemble.

Figure~\ref{fig:amort_feature_boxplots} shows the separation of key amortisation features between normal and abnormal loans, while Figure~\ref{fig:amort_short_mean_vs_70} illustrates how combining mean shortfall and the fraction of high-shortfall months concentrates anomalies in the upper-right region of the feature space.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_amort_feature_boxplots.pdf}
    \caption{Distribution of engineered amortisation features by class.
    Abnormal loans systematically exhibit higher mean shortfall and larger fractions of high-shortfall months, confirming that under-amortising loans are strongly associated with anomalies.}
    \label{fig:amort_feature_boxplots}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_amort_short_mean_vs_70.pdf}
    \caption{Scatter of amort\_short\_mean vs amort\_short\_70, coloured by class.
    Abnormal loans cluster in the upper-right region (large average shortfall and many months with extreme shortfall), providing an intuitive two-dimensional view of amortisation-based risk.}
    \label{fig:amort_short_mean_vs_70}
\end{figure}

\subsection{Unsupervised Detectors}
On top of the engineered feature space we train a diverse
set of unsupervised detectors on the training data only.

\subsubsection{Amortisation Score}
Using the amortisation features above, we define a simple weighted linear score that emphasises mean shortfall and the proportion of high-shortfall months.
This ``amort'' detector already reaches strong AUPRC on validation and becomes a cornerstone of our ensemble.

\subsubsection{Random-Forest Outlier Detection (RFOD)}
RFOD treats each feature in turn as a regression target and uses a Random Forest to predict it from the remaining features.
For each loan and target we compute the scaled absolute prediction error and aggregate across targets, with additional uncertainty weighting based on per-tree variation.
We train RFOD both on high-variance static+temporal features and on temporal features only, producing global and temporal RFOD scores that capture unusual interactions and trajectories.

\subsubsection{Local Outlier Factor (LOF)}
We train multiple LOF models with different neighbourhood sizes $k$ on the PCA embedding space.
This captures local density anomalies in a lower-dimensional representation.
Models are trained only on the training set, and scores are rank-normalised.

\subsubsection{Isolation Forest}
We train an Isolation Forest on the PCA embedding.
This tree-based method isolates anomalies by randomly partitioning the feature space and measuring path lengths in the forest.
As with all detectors, it is fitted only on the training set.

\subsubsection{KMeans + Cohort LOF}
We cluster loans in the PCA embedding space using KMeans (trained on training set).
For each sufficiently large cluster, we train a separate LOF model on the training loans within that cluster.
Validation/test loans are assigned to the nearest centroid and scored using the LOF model of their cluster.
This provides cohort-specific detection that compares each loan to similar peers.

\subsubsection{Random-Projection LOF (RP-LOF)}
Multiple LOF models are trained on random projections of the PCA embedding.
Their scores are rank-normalised and combined using max-pooling.
This helps detect anomalies that are visible only in certain subspaces and mitigates the curse of dimensionality.

\subsubsection{Multi-\texorpdfstring{$k$}{k} k-Distance}
For several neighbourhood sizes $k$, we compute the distance to the $k$-th nearest neighbour in the PCA space.
These distances are rank-normalised and averaged.
This provides a simple density-based signal that is complementary to LOF.

\subsubsection{Mahalanobis Distance}
We calculate the Mahalanobis distance in the scaled feature space (before PCA) using the training setâ€™s mean and a regularised covariance matrix.
This captures anomalies with respect to the global multivariate structure under an approximate Gaussian assumption.

\subsubsection{PCA Reconstruction Error}
We calculate the reconstruction error when projecting loans onto and back from the fitted PCA space.
High error indicates that the loan lies in a direction of low variance, potentially marking it as an outlier.

\subsection{Score Calibration}
Raw detector scores live on different scales and have different distributions.
To make them comparable, we apply the following calibration process for each detector $D$:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Empirical CDF (ECDF):} we fit an ECDF on the detector's scores for the training set, mapping a score $s$ to a probability $p = F_{\text{train}}(s)$.
    This ensures monotonicity (higher score $\rightarrow$ higher pseudo-probability).
    \item \textbf{Tail sharpening (optional):} for strong detectors such as amortisation we additionally experiment with simple extreme-value-style tail transformations (power $\gamma>1$) tuned on validation to sharpen separation among the highest-risk loans.
\end{enumerate}
The calibrated outputs lie in $[0,1]$ and can be meaningfully fused across detectors.

\subsection{Fusion Strategy}
We explored various fusion strategies to combine the calibrated detector scores.

\subsubsection{Rank-Based Fusion}
For each detector we convert scores to ranks and then:
\begin{itemize}[leftmargin=*]
    \item \textbf{Max / Mean rank:} take the maximum or average rank across detectors per loan;
    \item \textbf{Weighted average of ranks:} compute a weighted average using weights proportional to each detector's individual AUPRC on validation.
\end{itemize}
This rank-based approach is robust to scale differences and performed well in early experiments.

\subsubsection{Top-\texorpdfstring{$K$}{K} Rank Fusion}
We also explored \textbf{Top-$K$ fusion}, where we:
\begin{enumerate}[leftmargin=*]
    \item rank detectors by their individual validation AUPRC;
    \item select the top-$K$ detectors (e.g.\ $K=2$);
    \item fuse only their ranks using max or weighted average.
\end{enumerate}
A particularly strong rule was a weighted average of ranks of the top-2 detectors, denoted \textsf{rank::wavgr\_anktop2}.

\subsubsection{Amortisation-Gated Fusion}
As our amortisation detector emerged as both accurate and interpretable, we experimented with an \textbf{amortisation-gated} strategy:
we form a convex combination of all detector probabilities, but if the amortisation score exceeds a high threshold (e.g.\ 0.9--0.95), we allow it to dominate the fused score.
This blends the breadth of the ensemble with the high precision of the amortisation signal.

% ============================================================
\section{Experiments}
\subsection{Baselines}
Our initial experiments focused on classical anomaly detectors applied to a simpler feature space (basic cleaning plus standard scaling):
\begin{itemize}[leftmargin=*]
    \item Single LOF with tuned neighbourhood size.
    \item Multi-$k$ LOF ensemble.
    \item PCA reconstruction error.
    \item KMeans distance to nearest centroid.
    \item Isolation Forest.
\end{itemize}
The best baseline, a multi-$k$ LOF ensemble, achieved an AUPRC of roughly $0.22$ on validation, confirming that local density estimation is useful but leaving substantial room for improvement.

Table~\ref{tab:baseline_comparison} summarises the performance of key baseline configurations on the validation set.

\begin{table}[t]
\centering
\caption{Baseline Model Performance on Validation Set}
\label{tab:baseline_comparison}
\begin{tabular}{lcc}
\toprule
\textbf{Model Configuration} & \textbf{AUPRC} & \textbf{AUROC} \\
\midrule
Multi-$k$ LOF Ensemble & 0.2200 & 0.6100 \\
LOF ($k=50$, robust, no PCA) & 0.1955 & 0.5648 \\
LOF ($k=50$, robust, PCA 80) & 0.1550 & 0.5510 \\
Isolation Forest ($n=500$) & 0.1548 & 0.5273 \\
Elliptic Envelope & 0.1534 & 0.6206 \\
One-Class SVM (linear) & 0.1492 & 0.5497 \\
PCA Reconstruction Error & 0.1450 & 0.5400 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Iterative Improvement Journey}
Our final system emerged through several iterations:

\subsubsection{Step 1: Feature Engineering}
Introducing the domain-aware feature builder (sentinel handling, missing flags, static risk ratios, temporal windows, amortisation signals) significantly improved baseline detector performance.
\textbf{Even the simple LOF detector jumped from AUPRC $0.1955$ on raw features ($k=50$) to $0.3231$ on engineered features ($k=5$), representing a 65\% improvement purely from feature quality.}
This dramatic lift confirmed that domain knowledge encoded as features is more valuable than algorithmic sophistication alone.

\subsubsection{Step 2: Simple Ensemble}
Combining multiple detectors (LOF, Isolation Forest, PCA reconstruction error) using simple mean-of-ranks fusion boosted the score further (AUPRC around $0.30$), showing the benefit of heterogenous views on the data.

\subsubsection{Step 3: Amortisation Focus}
Realising the strength of the amortisation signal, we refined its calculation and began incorporating it explicitly into the fusion logic.
The amortisation detector alone reached AUPRC $0.4748$, making it a natural anchor for the ensemble.

\subsubsection{Step 4: Detector Diversification}
We added more detectors such as RFOD (global and temporal), KMeans+cohort LOF, random-projection LOF, multi-$k$ k-distance, Mahalanobis distance and improved histogram-style detectors.
This increased ensemble diversity and helped capture different anomaly types: regression-based, local density, correlation-based and global distance anomalies.

\subsubsection{Step 5: Refined Fusion}
We moved from simple averaging to more principled fusion.
Rank-based fusion with \textsf{rank::wavgr\_anktop2} (weighted average of ranks of the two best detectors by validation AUPRC) yielded strong performance.
The final system uses an amortisation-weighted gate strategy (\textsf{wgate::amort\_0.40::tau0.95}) that blends all detector probabilities but allows the amortisation signal to dominate when it exceeds a high threshold (0.95), achieving the best AUPRC while remaining highly interpretable.

\subsubsection{Step 6: Robust Scaling}
Switching from StandardScaler to RobustScaler improved performance for distance-based detectors, likely by reducing the influence of a few extreme points on feature scaling.
This change benefitted LOF, k-distance and Mahalanobis distance in particular.

\subsection{Detector-Level Performance}
Figure~\ref{fig:detector_auprc_bar} summarises the calibrated AUPRC and AUROC of our main detectors on the validation set.
The amortisation-based detector is the strongest individual component, but temporal RFOD, cohort LOF, RP-LOF and multi-$k$ distance each contribute complementary signal, justifying their inclusion in the fusion.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_detector_auprc_bar.pdf}
    \caption{Per-detector validation performance (AUPRC and AUROC).
    Amortisation dominates individually, but several other detectors achieve non-trivial AUPRC and provide useful diversity for the final ensemble.}
    \label{fig:detector_auprc_bar}
\end{figure}

\subsection{PR Curves and Capture Behaviour}
The impact of these design choices is visible in the precision--recall curves in Figure~\ref{fig:pr_baseline_amort_fusion}.
We compare:

\begin{itemize}[leftmargin=*]
    \item the baseline multi-$k$ LOF ensemble,
    \item the amortisation-only detector,
    \item the final fusion score.
\end{itemize}

The amortisation curve dominates the LOF baseline across most recall levels, showing the value of domain-aware features.
The fusion curve lies above amortisation alone in the mid-recall region, indicating that the ensemble recovers additional anomalies after the most obvious under-amortising loans have been flagged.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_pr_baseline_amort_fusion.pdf}
    \caption{Precision--recall curves on validation for baseline multi-$k$ LOF, amortisation-only detector and final fusion.
    The final ensemble clearly improves over both baselines, roughly doubling AUPRC compared to the initial LOF baseline.}
    \label{fig:pr_baseline_amort_fusion}
\end{figure}

Figure~\ref{fig:cum_capture_baseline_amort_fusion} shows the cumulative fraction of anomalies captured as we move down the ranked list of loans.
The fusion score captures a higher proportion of anomalies in the top few percent of loans than both the baseline and amortisation-only, which is particularly important for operational triage when only a small fraction of loans can be manually reviewed.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_cum_capture_baseline_amort_fusion.pdf}
    \caption{Cumulative anomaly capture rate versus fraction of portfolio reviewed.
    The fusion curve dominates both baseline and amortisation-only, indicating better prioritisation of anomalous loans among the highest-ranked candidates.}
    \label{fig:cum_capture_baseline_amort_fusion}
\end{figure}

We also examined performance across different LoanPurpose segments.
Figure~\ref{fig:auprc_by_loanpurpose} shows that the ensemble provides consistent improvements over the baseline across major segments such as purchase and refinance, suggesting that the model is not overly specialised to a single product type.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_auprc_by_loanpurpose.pdf}
    \caption{AUPRC by LoanPurpose segment for baseline multi-$k$ LOF, amortisation-only and final fusion.
    Segments are C: Cash-Out Refinance, N: No-Cash-Out Refinance, P: Purchase.
    The fusion method generally outperforms baselines across all segments, indicating robustness to product mix.}
    \label{fig:auprc_by_loanpurpose}
\end{figure}

\subsection{Final Results}
\label{sec:final_results}
The final system, using enhanced feature engineering, the diverse ensemble of detectors (amortisation, RFOD, several LOF variants, Isolation Forest, cohort LOF, RP-LOF, k-distance, Mahalanobis, PCA reconstruction) and a tuned fusion rule with RobustScaler, achieved:
\begin{itemize}[leftmargin=*]
    \item \textbf{Validation AUPRC:} $0.4981$;
    \item \textbf{Validation AUROC:} $0.7865$;
    \item \textbf{Kaggle Test AUPRC:} $0.478$.
\end{itemize}
This represents a substantial improvement over the best single-LOF baseline AUPRC of $0.1955$ and the multi-$k$ LOF ensemble baseline of $\sim 0.22$.

Table~\ref{tab:final_detector_performance} shows the validation performance of key individual detectors in our final ensemble, while Table~\ref{tab:baseline_comparison} in the Experiments section provides a detailed baseline comparison.
Table~\ref{tab:ablation_study} summarises the ablation study, showing the incremental contributions of feature engineering and ensemble fusion.

\begin{table}[t]
\centering
\caption{Final Ensemble: Individual Detector Performance on Validation Set}
\label{tab:final_detector_performance}
\begin{tabular}{lcc}
\toprule
\textbf{Detector} & \textbf{AUPRC} & \textbf{AUROC} \\
\midrule
Amortisation Score & 0.4748 & 0.7524 \\
Cohort LOF & 0.3242 & 0.7056 \\
Random Projection LOF & 0.3208 & 0.7056 \\
RFOD Temporal (TopQ) & 0.2891 & 0.6452 \\
RFOD Temporal & 0.2576 & 0.6453 \\
RFOD Global (TopQ) & 0.2496 & 0.6547 \\
RFOD Global & 0.2313 & 0.6553 \\
Multi-$k$ k-Distance & 0.1985 & 0.6184 \\
One-Class SVM & 0.1830 & 0.5744 \\
KMeans Distance & 0.1783 & 0.5594 \\
CAD Correlation (TopQ) & 0.1172 & 0.4629 \\
CAD Correlation & 0.1189 & 0.4722 \\
\midrule
\textbf{Final Fusion} & \textbf{0.4981} & \textbf{0.7865} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Ablation Study: Incremental Contribution of Key Components}
\label{tab:ablation_study}
\begin{tabular}{lcc}
\toprule
\textbf{Model Component} & \textbf{AUPRC} & \textbf{Improvement} \\
\midrule
Best Raw Baseline (LOF $k=50$) & 0.1955 & (Baseline) \\
+ Feature Engineering (LOF $k=5$) & 0.3231 & +65.3\% \\
+ Amortisation Detector & 0.4748 & +46.9\% \\
+ Full Ensemble Fusion & 0.4981 & +4.9\% \\
\midrule
\textbf{Total Improvement} & \textbf{0.4981} & \textbf{+154.8\%} \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================
\section{Unsupervised Explainability}
To gain insight into which features are most responsible for high anomaly scores without violating competition rules, we trained an Isolation Forest surrogate model on the training feature matrix only.
We then applied permutation importance to this surrogate, measuring the degradation in its ability to reproduce anomaly patterns when each feature is randomly shuffled.

We complement this with SHAP analysis on the surrogate to obtain local and global feature attributions.
Figure~\ref{fig:shap_summary_surrogate} shows a SHAP summary plot for the top engineered features.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_shap_summary_surrogate.pdf}
    \caption{SHAP summary plot for an Isolation Forest surrogate trained on the engineered feature space.
    Amortisation features, LTV/DTI-related measures and temporal dynamics dominate the explanation of anomaly scores, aligning with domain intuition.}
    \label{fig:shap_summary_surrogate}
\end{figure}

The analysis consistently highlighted:
\begin{itemize}[leftmargin=*]
    \item \textbf{Engineered amortisation features}: mean shortfall and fraction of high shortfall months were among the strongest drivers.
    \item \textbf{Temporal dynamics} of \textbf{InterestBearingUPB} and \textbf{EstimatedLTV}: especially their volatility and trend features across windows.
    \item \textbf{Static leverage indicators}: OriginalLTV, OriginalDTI, OriginalCLTV and MI\_Pct.
    \item \textbf{Program and product flags}: InterestOnlyFlag, BalloonIndicator, LoanPurpose and PropertyType, confirming the relevance of the amortisation logic and product segmentation.
\end{itemize}
Since our main ensemble uses the same engineered features, these results provide a coherent and domain-plausible explanation of what drives high anomaly scores.
A correlation heatmap of the key engineered features (Figure~\ref{fig:key_engineered_feature_corr}) in the Appendix further shows that these features capture complementary aspects of risk rather than being simple duplicates.

% ============================================================
\section{Lessons Learned}
We summarise the main lessons along four dimensions.

\subsection{Importance of Domain-Aware Features}
The largest performance jumps came not from switching algorithms but from building features that reflect mortgage finance intuition:
\begin{itemize}[leftmargin=*]
    \item Handling sentinel codes correctly prevents spurious signals in scaling and PCA.
    \item Ratios combining credit quality, leverage and payment burden are more informative than raw features alone.
    \item Amortisation-based signals summarise complex temporal behaviour in a way that is directly interpretable and highly predictive of anomalies.
    \item Temporal trend and volatility features convert raw monthly snapshots into compact descriptors of loan lifecycle dynamics.
\end{itemize}

A key part of our iterative process was identifying approaches that did not work.
For instance, as shown in Table~\ref{tab:baseline_comparison}, standard PCA dimensionality reduction hurt the performance of simple LOF baselines, and our early experiments showed that deep learning autoencoders (AUPRC $\sim$0.17) and standard Isolation Forest (AUPRC $\sim$0.15) significantly underperformed density-based and domain-specific methods.
This reinforced our focus on high-quality feature engineering over model complexity.

\subsection{Value of Heterogeneous Ensembles}
Combining detectors that operate on different principles (density-based like LOF, tree-based like Isolation Forest and RFOD, distance-based like Mahalanobis, and rule-based like amortisation shortfall) improves robustness.
No single method dominates across all loans, but a well-designed ensemble, especially with fusion rules that weight top performers and respect their strengths, substantially improves AUPRC.

\subsection{Working under Semi-Supervised Constraints}
The competition constraints forced us to clearly separate \emph{fitting} from \emph{selection}.
All components of the feature builder, scalers, PCA and detectors are fitted on normal training loans only; validation labels are used solely to tune hyperparameters, calibration and fusion.
This discipline reduces the risk of label leakage and mirrors real-world deployment conditions where abnormal labels are scarce.

\subsection{Explainability in an Unsupervised Setting}
Even without training a supervised model on labels, we can still provide explanations:
\begin{itemize}[leftmargin=*]
    \item Surrogate models such as Isolation Forest, combined with permutation importance and SHAP, reveal which engineered features most influence anomaly scores.
    \item The amortisation detector is inherently interpretable: high scores mean the borrower frequently pays down much less principal than a normal schedule would suggest.
\end{itemize}
Such explanations are essential for adoption in regulated financial institutions, where risk models must be transparent.

% ============================================================
\section{Conclusion}
We tackled a semi-supervised loan anomaly detection problem on a high-dimensional mortgage dataset, aiming to maximise AUPRC under strict label-usage constraints.
Starting from LOF-based baselines, we designed a domain-aware feature engineering pipeline and a heterogeneous ensemble of unsupervised detectors, calibrated and fused via rank-based and amortisation-aware strategies.
Our final system roughly doubles AUPRC compared to classical baselines while remaining interpretable through amortisation features and unsupervised importance analyses.

Future work could explore several directions.
First, deep representation learning on the temporal panels could capture more complex time-series patterns than our hand-crafted features.
Second, more principled meta-learning for fusion could automatically optimise detector weights rather than relying on validation AUPRC.
Third, our powerful amortisation detector relies on a mask (\texttt{amort\_mask\_not\_applicable}) to exclude non-standard loans (e.g., Interest-Only or Balloon); building specialised anomaly detectors for these specific loan types could further improve coverage, as their ``normal'' repayment behaviour is fundamentally different and may contain its own unique anomaly signatures.
Finally, online adaptation as new performance data arrives could enable the system to evolve with changing market conditions.

% ============================================================
\appendices
\section{Additional Diagnostic Figures}
\label{sec:appendix}

This appendix collects additional figures that provide further insight and diagnostics, but are not essential to the main narrative.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_key_engineered_feature_corr.pdf}
    \caption{Correlation heatmap of key engineered features (static risk ratios, amortisation features, and temporal trends/volatilities).
    The features are moderately correlated but not redundant, indicating that each adds distinct information to the anomaly detectors.}
    \label{fig:key_engineered_feature_corr}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_detector_corr_heatmap.pdf}
    \caption{Correlation heatmap of calibrated detector scores.
    Some detectors (e.g.\ LOF variants) are moderately correlated, but amortisation, temporal RFOD, cohort LOF and OCSVM maintain enough diversity to justify fusion.}
    \label{fig:detector_corr_heatmap}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_fusion_hist_by_class.pdf}
    \caption{Histogram of final fusion scores by class.
    Abnormal loans are shifted toward higher scores with a pronounced peak near 1.0, while normal loans show a broader distribution in the mid-range (0.5--0.7), supporting good class separation for operational decision thresholds.}
    \label{fig:fusion_hist_by_class}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_fusion_vs_amort_scatter.pdf}
    \caption{Scatter plot of final fusion score vs amortisation-only score.
    Most high-risk loans lie along the diagonal (driven by amortisation), but a subset of anomalies are picked up mainly by other detectors, illustrating the benefit of ensemble fusion.}
    \label{fig:fusion_vs_amort_scatter}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_pr_curves_grid.pdf}
    \caption{Grid of precision--recall curves for selected detectors.
    While no single detector dominates everywhere, the curves highlight complementary strengths across different recall regions.}
    \label{fig:pr_curves_grid}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_pr_baseline_amort_fusion.pdf}
    % already referenced in main text; repeated here for completeness
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_cum_capture_baseline_amort_fusion.pdf}
    % already referenced in main text; repeated here for completeness
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_detector_auprc_bar.pdf}
    % already referenced in main text; repeated here for completeness
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_amort_feature_boxplots.pdf}
    % already referenced in main text; repeated here for completeness
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_amort_short_mean_vs_70.pdf}
    % already referenced in main text; repeated here for completeness
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_auprc_by_loanpurpose.pdf}
    % already referenced in main text; repeated here for completeness
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig_shap_summary_surrogate.pdf}
    % already referenced in main text; repeated here for completeness
\end{figure}

% ============================================================
\begin{thebibliography}{9}

\bibitem{freddie}
Freddie Mac, ``Single-Family Loan-Level Dataset,'' 2019. [Online]. Available: \url{https://www.freddiemac.com/research/datasets/sf-loanlevel-dataset}

\bibitem{aggarwal2017}
C.~C. Aggarwal, \emph{Outlier Analysis}. Springer, 2017.

\bibitem{chandola2009}
V.~Chandola, A.~Banerjee, and V.~Kumar, ``Anomaly detection: A survey,'' \emph{ACM Computing Surveys}, vol.~41, no.~3, pp.~1--58, 2009.

\bibitem{ahmed2016}
M.~Ahmed, A.~N. Mahmood, and J.~Hu, ``A survey of network anomaly detection techniques,'' \emph{Journal of Network and Computer Applications}, vol.~60, pp.~19--31, 2016.

\bibitem{chugh2024}
A.~Chugh and P.~Bharti, ``A probabilistic approach driven credit card anomaly detection with CBLOF and isolation forest models,'' \emph{Alexandria Engineering Journal}, 2024.

\end{thebibliography}

\end{document}